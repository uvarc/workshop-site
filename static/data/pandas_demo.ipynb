{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Pandas Python Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The example data set\n",
    "This data set was downloaded from https://archive.ics.uci.edu/ml/datasets/Ozone+Level+Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from text files into pandas data structures\n",
    "The data is spread across multiple individual files. We'll be using two files for this example:\n",
    "- eighthr.data contains the data but without any column headers\n",
    "- eighthr.names containers the column headers (1 header/row) for the data file\n",
    "\n",
    "So we need to read both files separately and add the imported column headers to the pandas data structure when importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read file with column headers\n",
    "datainfo = pd.read_csv(\n",
    "    'eighthr.names',             # name of txt file to open\n",
    "    header=None,                     # this file does not have any column headers\n",
    "    skiprows=range(2),               # skip first two rows; they contain additional annotations that we dont need\n",
    "    names=['column name','data type'],    # set column labels\n",
    "    delimiter=':')                   # the ':' separates the column name from the data type in each row\n",
    "\n",
    "# fix column labels by adding missing label 'Ozone day' to columnnames\n",
    "columnnames = list(datainfo['column name'])\n",
    "columnnames.append('Ozone day')\n",
    "print(columnnames)\n",
    "\n",
    "# read data file and set column names\n",
    "data = pd.read_csv(\n",
    "    'eighthr.data',              # name of txt file to open\n",
    "    header=None,                     # this file does not have any column headers\n",
    "    names=columnnames,               # set column names\n",
    "    delimiter=',')                   # the ',' separates the column name from the data type in each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a basic overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe() function provides simple summary statistics about the underlying DataFrame object. For this particular unprocessed data set only rows 1-3 are meaningful, but at least we can get a quick idea of the total number of columns and number of values per data row (count, or # number of instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea regarding the single row values, we can use the .head() or .tail() functions, \n",
    "which output the first five or last five data rows respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print column labels and first five rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencing and inspecting a specific column\n",
    "Columns can be referenced via the column label or column index. If a single column is selected, the returned data type is a 1-dimensional DataSeries rather than a multi-dimensional DataFrame. We can store the returned data subset (selected column) in a variable.\n",
    "\n",
    "Here are three different approaches that all return the same column data as a DataSeries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select by column name\n",
    "precipitation = data['Precp']\n",
    "precipitation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select by column index; here we select column with index=72\n",
    "precipitation = data[data.columns[72]]\n",
    "precipitation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select by column index; here we select the second to last columnn (the last column has the index=-1)\n",
    "precipitation = data[data.columns[-2]]\n",
    "precipitation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting multiple columns and assigning them to a new variable\n",
    "We can use a list of column labels or column indices to extract a subset of the columns from the data set and store the subset in a new variable. When we select multiple columns, the selected data is returned as a multi-dimensional DataFrame.\n",
    "\n",
    "The order of the list elements determines the oder of the columns in the returned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with label 'Ozone day' and 'Precp'\n",
    "subset = data[['Date','Ozone day','Precp']]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns by index\n",
    "subset = data.iloc[:,[0,73,72]]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax for indexing a range of columns [startindex:endindex:direction]. Following Python list/array indexing and slicing convention, the endindex is excluded. direction=-1 enables reverse order, direction=1 indicates forward index order and can be ommitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the last (index=72) and second to last (index=71) column in reverse order\n",
    "subset = data[data.columns[73:71:-1]]\n",
    "subset.head()\n",
    "\n",
    "# ALTERNATIVE: use negative index (interpreted relative to end)\n",
    "# select the last (-1) and second to last (-2) column.\n",
    "subset = data[data.columns[-1:-3:-1]]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Preprocessing Steps\n",
    "### Dropping Columns\n",
    "Sometimes you find that not all columns are required for the data analysis. \n",
    "You can remove the unwanted columns with the drop() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping multiple columns by column label\n",
    "subset = data.drop(['SLP_','RH50'],axis=1)\n",
    "print('Original columns:\\n',data.columns.values)\n",
    "print('\\nRemaining columns after drop:\\n',subset.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Strings to Dates (datetime64 objects)\n",
    "The first column of our original data set contains the date as a string. We can convert the string representation \n",
    "to a datetime64 object, which will make selecting and grouping row data by specific date values (e.g. selection of \n",
    "first of each month, grouping by months or years) much easier (see section \"Grouping Data based on Datetime object values\").\n",
    "\n",
    "Note the last line of the output indicating that the data type (dtype) is now datetime64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before conversion\\n',data['Date'].head())\n",
    "data['Date'] = data['Date'].apply(pd.to_datetime, format=\"%m/%d/%Y\")\n",
    "print('\\nAfter conversion\\n',data['Date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Converting Values to Numbers (float)\n",
    "The first column of our original data set contains the data as a string (before we converted it to datetime objects). All other columns contain numeric values or '?' in case a particular value is missing. Due to the presence of the '?' the data type of columns 2 to 74 is a generic object. This precludes performing any mathematical oeprations on these values, like caluclating the mean or standard deviation for a group of rows. \n",
    "\n",
    "Let's fix that by selecting columns 2-73 (index 1 to the end) and forcing the conversion of all values in these columns to numbers. For values for which the conversion fails, the values will be set to 'NaN'. Note the replacement of '?' with 'NaN' in the last row of the data.head() output.\n",
    "\n",
    "'NaN' values will be excluded from any mathematical operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column values in place\n",
    "# Apply the pandas.to_numeric function to selected columns and reassign the converted columns to the original data.\n",
    "data[data.columns[1:]] = data[data.columns[1:]].apply(pd.to_numeric,errors='coerce')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output produced by data.decribe(). Note that we do not need to pass the argument \"include='all'\".\n",
    "\n",
    "Also note that the values for 'count' vary by column now, since count only considers column values that are not 'NaN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing data with missing values (NaN)\n",
    "After our conversion of the data types to a numeric type, we created NaN values in specif rows and/or columns. Sometimes you may wish to exclude the entire row/column with any NaN values, sometimes you may want to eliminate only those where an entire row/columns shows NaN values.  \n",
    "\n",
    "Pandas offers a convenient function for elminating rows or columns with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns where all values are NaN\n",
    "cleaned_data = data.dropna(axis=1, how='all')\n",
    "\n",
    "# Drop the columns where any value is NaN\n",
    "cleaned_data = data.dropna(axis=1, how='any')\n",
    "\n",
    "# Drop the rows where all of the values are NaN\n",
    "cleaned_data = data.dropna(axis=0, how='all')\n",
    "\n",
    "# Drop the rows where any of the values are NaN\n",
    "cleaned_data = data.dropna(axis=0, how='any')\n",
    "\n",
    "# Keep only the rows with at least 2 non-NaN values:\n",
    "cleaned_data = data.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting columns with 1/0 values to boolean type\n",
    "Note that the 'Ozone day' column contains the values 1 or 0, indicating whether the particular day is classified as an ozone day or not. We can convert the data of this column into the corresponding boolean values 'True' or 'False'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ozone day'] = data['Ozone day'].astype('bool')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering rows based on values in specific columns\n",
    "The data can be filtered based on a set of conditions. Different conditions can be applied to different columns. If multiple conditions are used, the individual conditions need to be enclosed in ( ).\n",
    "\n",
    "The returned DataFrame may have a different shape (number of rows) than the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data rows where the 'Precp' value is > 10 and the 'WSR0' value is < 3.\n",
    "filtered_data = data[(data['Precp'] > 10) & (data['WSR0'] < 3)]\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering and value masking with where()\n",
    "The where function allows masking of row values based on a set of conditions. \n",
    "There are two key differences to the filtering approach described above:\n",
    "- The returned data frame has the same shape as the input.\n",
    "- For rows where the specified condition evaluates to False, the original value is replaces by NaN or NaT, \n",
    "or a specified default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.where(data['WSR0'] < 3)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values in row 3 have been replaced with NaT ('Date' column) or NaN since the original data in this row does not meet the specified condition (data['WSR0'] < 3).\n",
    "\n",
    "Instead of having those rows be filled with 'NaN'/'NaT', we can also specify a value to replace the original value in cases where the condition evaluates to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep it simple, we're dropping the 'Date' column since setting 'Date' to -1 does not make much sense here\n",
    "filtered_data = data.drop(['Date'], axis=1).where(data['WSR0'] < 3,-1)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping rows by values in specific column\n",
    "In the example data, the 'Ozone day' column indicates wheter the specific day was considered a ozone day.\n",
    "\n",
    "Let's group the data rows based on whether they represent ozone days or not, and calculate the mean and std for the various measured wind speeds in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with column labels to groupby. We use a single column, but this can be a list with multiple column labels\n",
    "groupnames = ['Ozone day']\n",
    "\n",
    "# Create list with columns to select (cnames) which contains the data columns of interest and the column(s) that the \n",
    "# data should be grouped by\n",
    "datacolumns = ['Date','WSR0','WSR1','WSR2','WSR3','WSR4','WSR5']\n",
    "cnames = list(groupnames)\n",
    "cnames.extend(datacolumns)\n",
    "\n",
    "# Select columns based on cnames, create a group for each value found in the groupnames column(s)\n",
    "windspeed_by_ozone = data[cnames].groupby(groupnames)\n",
    "\n",
    "# In this case we have two groups: 'True' with data for ozone days, and 'False' witb data for non-ozone days.\n",
    "# Iterate over grouped data\n",
    "for name,group in windspeed_by_ozone:\n",
    "    print('Group name:',name,'\\n')\n",
    "    print('Data:\\n',group,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data based on Datetime object values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column in our data provides a Datetime object. It provides convenient attributes and functions to filter data by time/date or group data by periods, e.g. days, months, years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by month but combining all years --> 12 groups shown in rows\n",
    "data_by_calmonth = data.groupby(data['Date'].dt.month)\n",
    "data_by_calmonth.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by month for each calendar year\n",
    "data_by_date = data.groupby(data['Date'].dt.to_period('M'))\n",
    "data_by_date.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Multiple Columns\n",
    "We can specify multiple columns to be used as keys to group the data. The number of groups corresponds to the existing unique combination of value pairs in the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group the data by year (based on 'Date' column) and 'Ozone day'\n",
    "# Since we have 7 years and 2 'Ozone day' categories the data will be divided into 14 groups\n",
    "multi_group_data = data.groupby([data['Date'].dt.year,'Ozone day'])\n",
    "# Iterate over grouped data\n",
    "for name,group in multi_group_data:\n",
    "    print('Group name:',name)\n",
    "\n",
    "multi_group_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Aggregate Numbers (mean, standard deviation, min, max, etc.)\n",
    "Now that we have grouped the wind speed data based on the 'Ozone day' values, we can easily calculate \n",
    "some summary values for each data column in each group. Pandas DataFrames provide functions for easy calculation of many aggregate values, including:\n",
    "- count\n",
    "- mean\n",
    "- median\n",
    "- stdev\n",
    "- sum\n",
    "- min\n",
    "- max\n",
    "\n",
    "... and many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and std for each group\n",
    "ws_means = windspeed_by_ozone.mean()\n",
    "ws_stds = windspeed_by_ozone.std()\n",
    "\n",
    "ws_means.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the approach above, we store mean and standard deviation values in separate variables. Often it is convenient to collect the output of multiple aggregation functions in a single DataFrame. This can be done with the aggregate() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, std, min, max, count for each group and store aggregate values in single DataFrame\n",
    "ws_summary = windspeed_by_ozone.aggregate(['mean','std','min','max','count'])\n",
    "ws_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Data\n",
    "There are seral ways to plot data contained in a pandas DataFrame or DataSeries.\n",
    "### Using the pandas plotting functionality\n",
    "Simple plots can be created by calling plot.<PLOT_TYPE>() on a pandas DataFrame or group object. These functions/methods use the matplotlib package.\n",
    "\n",
    "Different plot types are provided by:\n",
    "- pandas.DataFrame.plot.area\n",
    "- pandas.DataFrame.plot.bar\n",
    "- pandas.DataFrame.plot.barh\n",
    "- pandas.DataFrame.plot.box\n",
    "- pandas.DataFrame.plot.density\n",
    "- pandas.DataFrame.plot.hexbin\n",
    "- pandas.DataFrame.plot.hist\n",
    "- pandas.DataFrame.plot.kde\n",
    "- pandas.DataFrame.plot.line\n",
    "- pandas.DataFrame.plot.pie\n",
    "- pandas.DataFrame.plot.scatter\n",
    "- pandas.DataFrame.boxplot\n",
    "- pandas.DataFrame.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plotting the wind speed means by 'Ozone day' group\n",
    "ws_means.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to show the two group values for a given wind speed column next to each other. \n",
    "That's easy, just transpose the values before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_means_trans = ws_means.transpose()\n",
    "ws_means_trans.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using matplotlib directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line is needed to show plot inline in notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Create a simple bar plot of means and std\n",
    "ws_stds_trans = ws_stds.transpose()\n",
    "ws_means_trans.plot.bar(yerr=ws_stds_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to get a quick glance at what data columns may or may not be correlated. \n",
    "We can use scatter_matrix for this. It expects a DataFrame and creates a grid of x-y scatter plots \n",
    "of all possible two column combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line is needed to show the plot inline in this notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# here we use a few columns of the ungrouped original data\n",
    "scatter_matrix(data[['WSR0','WSR1','WSR2','WSR3',]], figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a set of new scatter plots that displays 'WSR9' and 'Precp' grouped by year (based on 'Date' column values). Each plot will show the data for a single year grouped by 'Ozone day'. Here we use the matplotlib package directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line is needed to show the plot inline in this notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# import package for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "groups = [data['Date'].dt.year,'Ozone day']\n",
    "x = 'WSR9'\n",
    "y = 'Precp'\n",
    "grouped_data = data.groupby(groups)['Date','Ozone day',x,y]\n",
    "maxx = data[x].max()*1.2\n",
    "maxy = data[y].max()*1.2\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1,nrows=(len(grouped_data.groups)//2),figsize=(5,20))\n",
    "i = 0\n",
    "# create a plot for each year, each plot overlaying data for ozone days (True) and non-ozone days (False)\n",
    "for name, group in grouped_data:\n",
    "    row = i//2 # plotting two groups in each subplot\n",
    "    ax[row].plot(group[x], group[y], marker='o', linestyle='', ms=5, label=name)\n",
    "    ax[row].legend()\n",
    "    ax[row].set_xlim(0,maxx)\n",
    "    ax[row].set_ylim(0,maxy)\n",
    "    ax[row].set_xlabel(x)\n",
    "    ax[row].set_ylabel(y)\n",
    "    i+=1\n",
    "plt.tight_layout()    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
